{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121250,"databundleVersionId":14496514,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline","metadata":{}},{"cell_type":"markdown","source":"#### Loading the dataset","metadata":{}},{"cell_type":"code","source":"import datasets\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nds = datasets.load_dataset(\"csv\", data_files=\"/kaggle/input/autocorrect-aicc-round-1-2/train.csv\")\nds = ds[\"train\"].train_test_split(seed=42)\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.518433Z","iopub.status.idle":"2025-11-15T18:56:07.518863Z","shell.execute_reply.started":"2025-11-15T18:56:07.518632Z","shell.execute_reply":"2025-11-15T18:56:07.518653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = ds[\"train\"]\nval_ds = ds[\"test\"]\n\n# for later evaluation\nval_ds_input = val_ds.select_columns(\"misspell\")\nval_ds_solution = val_ds[\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.519609Z","iopub.status.idle":"2025-11-15T18:56:07.520006Z","shell.execute_reply.started":"2025-11-15T18:56:07.519812Z","shell.execute_reply":"2025-11-15T18:56:07.519831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame(val_ds)[\"text\"].str.len().describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.521462Z","iopub.status.idle":"2025-11-15T18:56:07.521895Z","shell.execute_reply.started":"2025-11-15T18:56:07.521657Z","shell.execute_reply":"2025-11-15T18:56:07.521675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame(val_ds)[\"misspell\"].str.len().describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.522847Z","iopub.status.idle":"2025-11-15T18:56:07.523219Z","shell.execute_reply.started":"2025-11-15T18:56:07.523031Z","shell.execute_reply":"2025-11-15T18:56:07.523049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Preparing the data","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, pre_tokenizers, processors\n\ntokenizer = Tokenizer(models.WordLevel(unk_token=\"<UNK>\"))\ntokenizer.pre_tokenizer = pre_tokenizers.Split(\"\", \"isolated\")\ntokenizer.enable_padding(pad_token=\"<PAD>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.524268Z","iopub.status.idle":"2025-11-15T18:56:07.524663Z","shell.execute_reply.started":"2025-11-15T18:56:07.524464Z","shell.execute_reply":"2025-11-15T18:56:07.524482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = tokenizer.model.get_trainer()\ntrainer.vocab_size = 1000\ntrainer.special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.525479Z","iopub.status.idle":"2025-11-15T18:56:07.525848Z","shell.execute_reply.started":"2025-11-15T18:56:07.525650Z","shell.execute_reply":"2025-11-15T18:56:07.525667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ds_iterator():\n    for row in train_ds:\n        yield row[\"text\"]\n        yield row[\"misspell\"]\n\ntokenizer.train_from_iterator(ds_iterator(), trainer=trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.527199Z","iopub.status.idle":"2025-11-15T18:56:07.527613Z","shell.execute_reply.started":"2025-11-15T18:56:07.527387Z","shell.execute_reply":"2025-11-15T18:56:07.527405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.post_processor = processors.TemplateProcessing(\n    single=\"<SOS> $0 <EOS>\",\n    special_tokens=[(\"<SOS>\", 2), (\"<EOS>\", 3)]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:07.528581Z","iopub.status.idle":"2025-11-15T18:56:07.529015Z","shell.execute_reply.started":"2025-11-15T18:56:07.528803Z","shell.execute_reply":"2025-11-15T18:56:07.528822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\ntokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\ntokenizer.add_special_tokens({\"pad_token\": \"<PAD>\", \"unk_token\": \"<UNK>\", \"cls_token\": \"<SOS>\", \"eos_token\": \"<EOS>\"})\ntokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:08.357671Z","iopub.execute_input":"2025-11-15T18:56:08.358505Z","iopub.status.idle":"2025-11-15T18:56:17.728128Z","shell.execute_reply.started":"2025-11-15T18:56:08.358466Z","shell.execute_reply":"2025-11-15T18:56:17.726397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_fn(examples):\n    input_tokens = tokenizer(examples['misspell'], padding='max_length', truncation=True, max_length=768, return_tensors='pt')['input_ids']\n    label_tokens = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=768, return_tensors='pt')['input_ids']\n\n    input_lengths = (input_tokens != tokenizer.pad_token_id).sum(dim=1)\n    label_lengths = (label_tokens != tokenizer.pad_token_id).sum(dim=1)\n\n    return {\n        'input_ids': input_tokens,\n        'labels': label_tokens,\n        'input_lengths': input_lengths,\n        'label_lengths': label_lengths,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:17.728739Z","iopub.status.idle":"2025-11-15T18:56:17.729020Z","shell.execute_reply.started":"2025-11-15T18:56:17.728893Z","shell.execute_reply":"2025-11-15T18:56:17.728906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=['text', 'misspell'])\ntrain_ds.set_format(\"torch\")\nval_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=['text', 'misspell'])\nval_ds.set_format(\"torch\")\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:17.730528Z","iopub.status.idle":"2025-11-15T18:56:17.730939Z","shell.execute_reply.started":"2025-11-15T18:56:17.730747Z","shell.execute_reply":"2025-11-15T18:56:17.730765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Building the model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass LSTMAutocorrect(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, lens):\n        x = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lens.cpu(), batch_first=True, enforce_sorted=False)\n        packed_outputs, (_, _) = self.lstm(packed)\n        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:23.706025Z","iopub.execute_input":"2025-11-15T18:56:23.706435Z","iopub.status.idle":"2025-11-15T18:56:23.713522Z","shell.execute_reply.started":"2025-11-15T18:56:23.706407Z","shell.execute_reply":"2025-11-15T18:56:23.712663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size\nembed_size = 64\nhidden_size = 128\nnum_epochs = 10\nlearning_rate = 0.001\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize model, loss function, and optimizer\nmodel = LSTMAutocorrect(vocab_size, embed_size, hidden_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:27.426380Z","iopub.execute_input":"2025-11-15T18:56:27.426745Z","iopub.status.idle":"2025-11-15T18:56:27.441136Z","shell.execute_reply.started":"2025-11-15T18:56:27.426694Z","shell.execute_reply":"2025-11-15T18:56:27.439517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        lengths = batch['input_lengths'].to(device)\n\n        outputs = model(input_ids, lengths)\n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_sentence(model, tokenizer, input_ids, device=\"cpu\"):\n    \"\"\"\n    input_ids: tensor of shape (1, seq_len) with tokenized input\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        input_ids = input_ids.to(device)\n        outputs = model(input_ids, torch.tensor([input_ids.size(1)], dtype=torch.int64))  # (1, seq_len, vocab_size)\n        predictions = outputs.argmax(dim=-1)  # (1, seq_len)\n\n    # Convert ids back to tokens / string\n    predicted_tokens = tokenizer.convert_ids_to_tokens(predictions[0][1:-1].tolist())\n    return \"\".join(predicted_tokens)\n\n# Suppose you have a test sentence\ntest_text = \" Good morning rveryone. How are you. \"  # misspelled input\n\n# Tokenize and convert to tensor\ninput_ids = tokenizer.encode(test_text, return_tensors=\"pt\")\n\n# Get prediction\ncorrected = predict_sentence(model, tokenizer, input_ids, device=device)\nprint(\"Input:\", test_text)\nprint(\"Corrected:\", corrected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:56:49.093534Z","iopub.execute_input":"2025-11-15T18:56:49.094066Z","iopub.status.idle":"2025-11-15T18:56:49.114220Z","shell.execute_reply.started":"2025-11-15T18:56:49.094023Z","shell.execute_reply":"2025-11-15T18:56:49.112992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### Predicting on Test","metadata":{}},{"cell_type":"code","source":"test_ds = datasets.load_dataset(\"csv\", data_files=\"/kaggle/input/autocorrect-aicc-round-1-2/test.csv\")\n# test_ds = val_ds_input # for validation\ntest_ds = test_ds[\"train\"] # for test\ntest_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_test_fn(examples):\n    input_tokens = tokenizer(examples['misspell'], padding='max_length', truncation=True, max_length=4096, return_tensors='pt')['input_ids']\n    input_lengths = (input_tokens != tokenizer.pad_token_id).sum(dim=1)\n\n    return {\n        'input_ids': input_tokens,\n        'input_lengths': input_lengths\n    }\n\ntest_ds = test_ds.map(tokenize_test_fn, batched=True, remove_columns=['misspell'])\ntest_ds.set_format(\"torch\")\n\ntest_loader = DataLoader(test_ds, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wrap_model_prediction(model, batch, device):\n    input_ids = batch['input_ids'].to(device)\n    lengths = batch['input_lengths'].to(device)\n\n    return model(input_ids, lengths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note: For this task, the requirement for the evaluation to finish in less than 250 seconds will be based on this block of code below. DO NOT MODIFY ANY CODE - use the wrapper function above for any changes due to your model architecture.** Please don't try anything that goes against the spirit of this challenge...","metadata":{}},{"cell_type":"code","source":"%%time\ndevice = \"cpu\" # change to cuda for validation\npreds_all = []\n\nmodel.eval()\nmodel.to(device)\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        outputs = wrap_model_prediction(model, batch, device)\n        predictions = outputs.argmax(dim=-1)\n        preds_all.append(predictions.cpu())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = [] # convert tokens back to string, excluded from timed evaluation as this takes quite a while\nfor pred in tqdm(preds_all):\n    results += [ \"\".join(tokenizer.convert_ids_to_tokens(x, skip_special_tokens=True)) for x in pred ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(results)\ndf = df.reset_index()\ndf.columns = [\"id\", \"corrected\"]\ndf.to_csv(\"/kaggle/working/test_submission.csv\", index=False)\n\nprint(\"test_submission.csv generated!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Code for Evaluation (for Validation)\nUnfortunately, due to environment restrictions, evaluation on the server uses the Python-based implementation provided by `torchmetrics` instead of the much faster implementation provided by `jiwer`. Hence, results may vary slightly, and expect server-based eval to take about 10 minutes.","metadata":{}},{"cell_type":"code","source":"#! pip install jiwer evaluate --quiet\n#import evaluate\n\n#cer = evaluate.load(\"cer\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pandas as pd\n\n#submission = pd.read_csv(\"test_submission.csv\")\n# solution = pd.DataFrame(pd.Series(val_ds_solution)) # for validation\n#solution = pd.read_csv(\"test_sol.csv\") # for testing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cer.compute(\n#    predictions=submission[\"corrected\"],\n#    references=solution.iloc[:, 0]\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}