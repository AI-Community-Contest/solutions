{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a230344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fa2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd0ceda8af0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "root_dir = \"./face-matching\"\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474de59",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d30976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference IDs: ['048', '025', '095', '043', '105', '071', '046', '096', '020', '085', '061', '073', '084', '026', '008']\n"
     ]
    }
   ],
   "source": [
    "ref_df = pd.read_csv(\"ref_img.csv\", dtype={\"ref_img\": str})\n",
    "ref_ids = ref_df[\"ref_img\"].tolist()\n",
    "print(f\"Reference IDs: {ref_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56f538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\", use_fast=True\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1fb578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = sorted([f[:-4] for f in os.listdir(f\"{root_dir}/images\") if f.endswith(\".jpg\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7ce3c",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72b6d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:03<00:00, 32.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 109 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "for img_id in tqdm(all_images):\n",
    "    img_path = f\"{root_dir}/images/{img_id}.jpg\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = np.array(img)\n",
    "    face_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face_pil = Image.fromarray(face_img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=face_pil, return_tensors=\"pt\").to(device)\n",
    "        feature = model.get_image_features(**inputs)\n",
    "        features[img_id] = feature.cpu().numpy()\n",
    "\n",
    "print(f\"Extracted features for {len(features)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b29f47",
   "metadata": {},
   "source": [
    "Find top 5 similar images for each reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11afb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 543.51it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for ref_id in tqdm(ref_ids):\n",
    "    ref_feature = features.get(ref_id)\n",
    "    if ref_feature is None:\n",
    "        continue\n",
    "\n",
    "    # calculate cosine similarity with all images\n",
    "    similarities = {}\n",
    "    for img_id, feature in features.items():\n",
    "        sim = np.dot(ref_feature[0], feature[0]) / (\n",
    "            np.linalg.norm(ref_feature[0]) * np.linalg.norm(feature[0]) + 1e-8\n",
    "        )\n",
    "        similarities[img_id] = float(sim)\n",
    "\n",
    "    # sort by similarity, exclude reference image, take top 5\n",
    "    sorted_ids = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_5 = [img_id for img_id, _ in sorted_ids if img_id != ref_id][:5]\n",
    "\n",
    "    results.append({\"ref_img\": ref_id, \"photos\": \"|\".join(top_5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc29ef",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a0facfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_img</th>\n",
       "      <th>photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>048</td>\n",
       "      <td>101|102|073|093|039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>025</td>\n",
       "      <td>041|006|083|067|033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>095</td>\n",
       "      <td>014|021|047|097|072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043</td>\n",
       "      <td>034|027|090|013|079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>024|029|023|104|046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>071</td>\n",
       "      <td>070|011|017|065|088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>046</td>\n",
       "      <td>060|019|010|069|056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>096</td>\n",
       "      <td>035|098|018|068|015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>020</td>\n",
       "      <td>031|053|077|091|038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>085</td>\n",
       "      <td>106|009|037|103|033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>061</td>\n",
       "      <td>104|062|059|107|002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>073</td>\n",
       "      <td>003|090|039|044|108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>084</td>\n",
       "      <td>012|089|079|007|055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>026</td>\n",
       "      <td>051|080|099|076|004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>008</td>\n",
       "      <td>000|054|087|028|064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ref_img               photos\n",
       "0      048  101|102|073|093|039\n",
       "1      025  041|006|083|067|033\n",
       "2      095  014|021|047|097|072\n",
       "3      043  034|027|090|013|079\n",
       "4      105  024|029|023|104|046\n",
       "5      071  070|011|017|065|088\n",
       "6      046  060|019|010|069|056\n",
       "7      096  035|098|018|068|015\n",
       "8      020  031|053|077|091|038\n",
       "9      085  106|009|037|103|033\n",
       "10     061  104|062|059|107|002\n",
       "11     073  003|090|039|044|108\n",
       "12     084  012|089|079|007|055\n",
       "13     026  051|080|099|076|004\n",
       "14     008  000|054|087|028|064"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(results)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2336689",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f\"{root_dir}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
