{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3c16607",
   "metadata": {
    "id": "d3c16607"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1546c6ac",
   "metadata": {
    "id": "1546c6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./face-matching\"\n",
    "ROOT_PATH = \"./face-matching\"\n",
    "IMG_DIR = os.path.join(ROOT_PATH, \"images\")\n",
    "REF_CSV = os.path.join(ROOT_PATH, \"ref_img.csv\")\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
    "THRESHOLD = 0.80\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ab416",
   "metadata": {
    "id": "153ab416"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b4b8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv(REF_CSV, dtype={'ref_img': str})\n",
    "ref_ids = ref_df[\"ref_img\"].tolist()\n",
    "all_images = [f for f in os.listdir(IMG_DIR) if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c7b32",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e377dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features with TTA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e7ca0fb388476a9cd3823156390095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {}\n",
    "print(\"Extracting features with TTA...\")\n",
    "\n",
    "for img_filename in tqdm(all_images):\n",
    "    img_id = img_filename[:-4]\n",
    "    img_path = os.path.join(IMG_DIR, img_filename)\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # This helps if the face is angled or partially hidden\n",
    "        images_to_process = [image, image.transpose(Image.FLIP_LEFT_RIGHT)]\n",
    "        \n",
    "        inputs = processor(images=images_to_process, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_features = model.get_image_features(**inputs)\n",
    "            batch_features = batch_features / batch_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "            # Average the original and flipped features\n",
    "            avg_feature = batch_features.mean(dim=0)\n",
    "            avg_feature = avg_feature / avg_feature.norm(p=2, dim=-1, keepdim=True)\n",
    "            \n",
    "        features[img_id] = avg_feature.cpu().numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error {img_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "print(\"Matching...\")\n",
    "for ref_id in ref_ids:\n",
    "    if ref_id not in features:\n",
    "        results.append({\"ref_img\": ref_id, \"photos\": \"\"})\n",
    "        continue\n",
    "\n",
    "    ref_feature = features[ref_id]\n",
    "    scores = {}\n",
    "    \n",
    "    for img_id, feature in features.items():\n",
    "        if img_id == ref_id: continue\n",
    "        \n",
    "        # score\n",
    "        score = np.dot(ref_feature, feature.T).item()\n",
    "        scores[img_id] = score\n",
    "    \n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    matches = [img_id for img_id, score in sorted_scores if score > THRESHOLD]\n",
    "    \n",
    "    # Prevent null columns and ensure at least 4 matches\n",
    "    if len(matches) < 4:\n",
    "        matches = [img_id for img_id, _ in sorted_scores[:4]]\n",
    "        \n",
    "    results.append({\n",
    "        \"ref_img\": ref_id, \n",
    "        \"photos\": \"|\".join(matches)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d5a91",
   "metadata": {},
   "source": [
    "# Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features with TTA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55b0fc049a54b9a9430a72be0159921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ref_img                           photos\n",
      "0      048                  101|102|078|066\n",
      "1      025  083|086|006|067|040|041|033|042\n",
      "2      095          021|097|014|047|072|045\n",
      "3      043          034|027|013|023|081|052\n",
      "4      105  024|029|093|050|058|100|001|057\n",
      "5      071  011|070|065|088|017|049|032|094\n",
      "6      046          060|019|010|069|075|063\n",
      "7      096              035|015|068|018|098\n",
      "8      020              031|053|077|091|038\n",
      "9      085              106|103|036|009|037\n",
      "10     061      104|107|059|062|002|022|030\n",
      "11     073          090|003|039|108|044|082\n",
      "12     084      012|089|055|056|007|016|079\n",
      "13     026              080|051|099|004|076\n",
      "14     008  087|000|064|074|054|092|028|005\n",
      "\n",
      "Counts per reference: with total matches = 94\n",
      "Ref 048: 4 matches\n",
      "Ref 025: 8 matches\n",
      "Ref 095: 6 matches\n",
      "Ref 043: 6 matches\n",
      "Ref 105: 8 matches\n",
      "Ref 071: 8 matches\n",
      "Ref 046: 6 matches\n",
      "Ref 096: 5 matches\n",
      "Ref 020: 5 matches\n",
      "Ref 085: 5 matches\n",
      "Ref 061: 7 matches\n",
      "Ref 073: 6 matches\n",
      "Ref 084: 7 matches\n",
      "Ref 026: 5 matches\n",
      "Ref 008: 8 matches\n"
     ]
    }
   ],
   "source": [
    "img_features = {} \n",
    "ref_features = {}\n",
    "\n",
    "print(\"Extracting features with TTA...\")\n",
    "for img_filename in tqdm(all_images):\n",
    "    img_id = img_filename[:-4]\n",
    "    img_path = os.path.join(IMG_DIR, img_filename)\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        images = [image, image.transpose(Image.FLIP_LEFT_RIGHT)]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb = model.get_image_features(**inputs)\n",
    "            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Average original and flipped\n",
    "            emb = emb.mean(dim=0)\n",
    "            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            \n",
    "        feat = emb.cpu().numpy()\n",
    "        img_features[img_id] = feat\n",
    "        \n",
    "        if img_id in ref_ids:\n",
    "            ref_features[img_id] = feat\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error {img_id}: {e}\")\n",
    "\n",
    "non_ref_ids = [i for i in img_features.keys() if i not in ref_ids]\n",
    "\n",
    "img_matrix = np.vstack([img_features[i] for i in non_ref_ids])\n",
    "ref_matrix = np.vstack([ref_features[r] for r in ref_ids])\n",
    "\n",
    "# Shape: (N_images, 15)\n",
    "raw_scores = np.dot(img_matrix, ref_matrix.T)\n",
    "\n",
    "means = raw_scores.mean(axis=0)\n",
    "stds = raw_scores.std(axis=0)\n",
    "z_scores = (raw_scores - means) / (stds + 1e-6) # Avoid div by zero\n",
    "\n",
    "final_assignments = {r: [] for r in ref_ids}\n",
    "assigned_img_indices = set()\n",
    "\n",
    "candidates = []\n",
    "rows, cols = z_scores.shape\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        candidates.append((z_scores[r, c], r, c))\n",
    "\n",
    "candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "MAX_CAP = 8 \n",
    "matches_count = {c: 0 for c in range(cols)}\n",
    "\n",
    "# Pass 1: Fill up to Cap OR ELSE ITS GONNA GET DOMINATED BY 1 lol\n",
    "for score, img_idx, ref_idx in candidates:\n",
    "    if img_idx in assigned_img_indices:\n",
    "        continue\n",
    "    \n",
    "    if matches_count[ref_idx] < MAX_CAP:\n",
    "        real_img_id = non_ref_ids[img_idx]\n",
    "        real_ref_id = ref_ids[ref_idx]\n",
    "        final_assignments[real_ref_id].append(real_img_id)\n",
    "        \n",
    "        assigned_img_indices.add(img_idx)\n",
    "        matches_count[ref_idx] += 1\n",
    "\n",
    "# Assign them to their absolute best match regardless of cap\n",
    "for i in range(rows):\n",
    "    if i not in assigned_img_indices:\n",
    "        best_ref_idx = np.argmax(z_scores[i])\n",
    "        real_img_id = non_ref_ids[i]\n",
    "        real_ref_id = ref_ids[best_ref_idx]\n",
    "        final_assignments[real_ref_id].append(real_img_id)\n",
    "\n",
    "results = []\n",
    "for rid in ref_ids:\n",
    "    photos = final_assignments[rid]\n",
    "    results.append({\n",
    "        \"ref_img\": rid,\n",
    "        \"photos\": \"|\".join(photos)\n",
    "    })\n",
    "\n",
    "submission = pd.DataFrame(results)\n",
    "print(submission)\n",
    "\n",
    "# Sanity Check: Print lengths\n",
    "count = sum(len(v) for v in final_assignments.values())\n",
    "\n",
    "print(\"\\nCounts per reference: with total matches =\", count)\n",
    "for rid in ref_ids:\n",
    "    count = len(final_assignments[rid])\n",
    "    print(f\"Ref {rid}: {count} matches\")\n",
    "    \n",
    "submission.to_csv(\"submission_zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259f52e",
   "metadata": {},
   "source": [
    "# Hungarian Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 109\n",
      "References: 15\n",
      "Targets to assign: 94\n",
      "Extracting features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f579b51fa7eb44329cbff1d4780e1f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Refs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340bffee07254292855e392189979794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving assignment problem on matrix shape (94, 150)...\n",
      "\n",
      "Final Counts:\n",
      "Ref 048: 4\n",
      "Ref 025: 10\n",
      "Ref 095: 6\n",
      "Ref 043: 6\n",
      "Ref 105: 10\n",
      "Ref 071: 7\n",
      "Ref 046: 6\n",
      "Ref 096: 5\n",
      "Ref 020: 5\n",
      "Ref 085: 5\n",
      "Ref 061: 5\n",
      "Ref 073: 6\n",
      "Ref 084: 7\n",
      "Ref 026: 5\n",
      "Ref 008: 7\n",
      "Saved to submission_hungarian.csv\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "target_images = [img for img in all_images if img[:-4] not in ref_ids]\n",
    "\n",
    "print(f\"Total Images: {len(all_images)}\")\n",
    "print(f\"References: {len(ref_ids)}\")\n",
    "print(f\"Targets to assign: {len(target_images)}\")\n",
    "\n",
    "target_feats = []\n",
    "target_ids = []\n",
    "ref_feats_map = {}\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "\n",
    "def get_embedding(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    images = [image, image.transpose(Image.FLIP_LEFT_RIGHT)]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "        emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "        emb = emb.mean(dim=0) # Average them\n",
    "        emb = emb / emb.norm(p=2, dim=-1, keepdim=True) # Renormalize\n",
    "\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "for rid in tqdm(ref_ids, desc=\"Refs\"):\n",
    "    path = os.path.join(IMG_DIR, f\"{rid}.jpg\")\n",
    "    if os.path.exists(path):\n",
    "        ref_feats_map[rid] = get_embedding(path)\n",
    "    else:\n",
    "        print(f\"WARNING: Ref {rid} not found!\")\n",
    "\n",
    "for img_filename in tqdm(target_images, desc=\"Targets\"):\n",
    "    img_id = img_filename[:-4]\n",
    "    path = os.path.join(IMG_DIR, img_filename)\n",
    "    try:\n",
    "        feat = get_embedding(path)\n",
    "        target_feats.append(feat)\n",
    "        target_ids.append(img_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error {img_id}: {e}\")\n",
    "\n",
    "# Shape: (N_targets, 512)\n",
    "target_matrix = np.vstack(target_feats)\n",
    "# Shape: (15, 512)\n",
    "ref_matrix = np.vstack([ref_feats_map[rid] for rid in ref_ids])\n",
    "\n",
    "# Shape: (N_targets, 15)\n",
    "raw_sims = np.dot(target_matrix, ref_matrix.T)\n",
    "\n",
    "means = raw_sims.mean(axis=0)\n",
    "stds = raw_sims.std(axis=0)\n",
    "z_scores = (raw_sims - means) / (stds + 1e-6) # Same as above\n",
    "\n",
    "# Hungarian Algo\n",
    "# We need to assign N_targets to 15 references.\n",
    "# But Hungarian is 1-to-1.\n",
    "# Solution: Duplicate the references into \"Slots\".\n",
    "# If we have ~94 targets and 15 refs, avg is ~6. \n",
    "# We give each ref 10 slots to be safe.\n",
    "SLOTS_PER_REF = 10 \n",
    "total_slots = len(ref_ids) * SLOTS_PER_REF\n",
    "\n",
    "# Create Cost Matrix\n",
    "# Hungarian minimizes cost, so Cost = -ZScore\n",
    "cost_matrix = np.zeros((len(target_ids), total_slots))\n",
    "\n",
    "for i in range(len(ref_ids)):\n",
    "    score_col = z_scores[:, i]\n",
    "    \n",
    "    start_col = i * SLOTS_PER_REF\n",
    "    end_col = start_col + SLOTS_PER_REF\n",
    "    \n",
    "    # negative to maximize\n",
    "    cost_matrix[:, start_col:end_col] = -score_col[:, np.newaxis]\n",
    "\n",
    "print(f\"Solving assignment problem on matrix shape {cost_matrix.shape}...\")\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "clusters = {rid: [] for rid in ref_ids}\n",
    "\n",
    "for r, c in zip(row_ind, col_ind):\n",
    "    ref_idx = c // SLOTS_PER_REF\n",
    "    \n",
    "    assigned_ref_id = ref_ids[ref_idx]\n",
    "    assigned_img_id = target_ids[r]\n",
    "    \n",
    "    clusters[assigned_ref_id].append(assigned_img_id)\n",
    "\n",
    "results = []\n",
    "print(\"\\nFinal Counts:\")\n",
    "for rid in ref_ids:\n",
    "    photos = clusters[rid]\n",
    "    print(f\"Ref {rid}: {len(photos)}\")\n",
    "    results.append({\n",
    "        \"ref_img\": rid, \n",
    "        \"photos\": \"|\".join(photos)\n",
    "    })\n",
    "\n",
    "submission = pd.DataFrame(results)\n",
    "submission.to_csv(\"submission_hungarian.csv\", index=False)\n",
    "print(\"Saved to submission_hungarian.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
